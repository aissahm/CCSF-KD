# FedCCSKD - Federated Clustered Client Selection and Knowledge Distillation training algorithm

The main goal of FedCCSKD is to decrease the overall communication costs. It is an innovative combination of: (i) client selection, and (ii) knowledge distillation approaches with three main objectives: (i) reducing the number of devices training at every round; (ii) increasing convergence speed; and (iii) mitigating the effect of clientsâ€™ heterogeneous data on the global model effectiveness.

### From the paper: 
@INPROCEEDINGS{10733694,
  author={Mohamed, Aissa H. and Da Costa, Joahannes B. D. and Villas, Leandro A. and Dos Reis, Julio C. and de Souza, Allan M.},
  booktitle={2024 IEEE Symposium on Computers and Communications (ISCC)}, 
  title={Combining Client Selection Strategy with Knowledge Distillation for Federated Learning in non-IID Data}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  keywords={Training;Accuracy;Costs;Federated learning;Computational modeling;Scalability;Diversity reception;Data models;Servers;Convergence},
  doi={10.1109/ISCC61673.2024.10733694}}
