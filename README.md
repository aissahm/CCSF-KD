# FedCCSKD - Federated Clustered Client Selection and Knowledge Distillation training algorithm

The main goal of FedCCSKD is to decrease the overall communication costs. It is an innovative combination of: (i) client selection, and (ii) knowledge distillation approaches with three main objectives: (i) reducing the number of devices training at every round; (ii) increasing convergence speed; and (iii) mitigating the effect of clientsâ€™ heterogeneous data on the global model effectiveness.

### From the paper: 
A. H. Mohamed, J. B. D. Da Costa, L. A. Villas, J. C. Dos Reis and A. M. de Souza, "Combining Client Selection Strategy with Knowledge Distillation for Federated Learning in non-IID Data," 2024 IEEE Symposium on Computers and Communications (ISCC), Paris, France, 2024, pp. 1-7, doi: 10.1109/ISCC61673.2024.10733694.
keywords: {Training;Accuracy;Costs;Federated learning;Computational modeling;Scalability;Diversity reception;Data models;Servers;Convergence},
