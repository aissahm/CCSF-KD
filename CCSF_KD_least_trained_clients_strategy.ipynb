{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnMn7JuNr2SW38U5+3RjR3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aissahm/CCSF-KD/blob/main/CCSF_KD_least_trained_clients_strategy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CCSF-KD, least trained client strategy**"
      ],
      "metadata": {
        "id": "qcXwC6WY0E4N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Wvy9UCZzUB2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import random as random\n",
        "from tensorflow.keras import layers, models, losses, optimizers\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#given the dataset X, Y, the object with indexes for every client, returns the dataset of client identified with its client_id\n",
        "def returnClientDataset(client_id, clients_data_obj, x, y):\n",
        "  dataset_indexes = np.array(clients_data_obj[client_id][\"indexes\"])\n",
        "  return [x[dataset_indexes], y[dataset_indexes]]\n",
        "\n",
        "def returnClientGradientAsVector(client_gradient):\n",
        "  weights = []\n",
        "  for weight in client_gradient:\n",
        "      weight = weight.reshape(weight.size)\n",
        "      weights.extend( weight)\n",
        "  return np.array(weights)\n",
        "\n",
        "def returnClusterID(client_id, cluster_obj):\n",
        "  for attr, value in cluster_obj.items():\n",
        "    if client_id in cluster_obj[attr][\"cluster\"]:\n",
        "      return attr\n",
        "  return -1\n",
        "\n",
        "def returnClientWeight(client_id, cluster_obj):\n",
        "  cluster_id = returnClusterID(client_id, cluster_obj)\n",
        "  return cluster_obj[cluster_id][\"cluster_size\"]"
      ],
      "metadata": {
        "id": "1V7qfONn0EWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def knowledge_distillation_loss(y_true, y_pred):\n",
        "  y_true = tf.convert_to_tensor(y_true, dtype=tf.float32)\n",
        "\n",
        "  # Ensure that y_pred has the same shape as soft targets\n",
        "  y_pred = tf.convert_to_tensor(y_pred, dtype=tf.float32)\n",
        "\n",
        "  loss_ce = losses.categorical_crossentropy(y_true, y_pred, from_logits=False)\n",
        "  return loss_ce\n",
        "\n",
        "def returnInitialGlobalModel():\n",
        "  model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation=\"softmax\"),\n",
        "    ]\n",
        "  )\n",
        "  #model.summary()\n",
        "  model.compile(loss=knowledge_distillation_loss, optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "  return model\n",
        "\n",
        "#returns a copy of the global model to client\n",
        "def returnCopyGlobalModelToClient(globalmodel):\n",
        "  clientmodel = returnInitialGlobalModel()\n",
        "  clientmodel.set_weights(globalmodel.get_weights())\n",
        "  return clientmodel\n",
        "\n",
        "#Given the weights after training and initial weights, returns the gradient from entire training\n",
        "def computeClientGradient(modelNotTrained, modelTrained):\n",
        "  gradient = []\n",
        "  notTrainedWeight = modelNotTrained.get_weights()\n",
        "  i = 0\n",
        "  for weight in modelTrained.get_weights():\n",
        "    gradient.append( notTrainedWeight[i] - weight )\n",
        "    i += 1\n",
        "  return gradient\n",
        "\n",
        "#add the client gradient to the global model\n",
        "def addGradientNoCompression(modelNotTrained, gradient, clientweight):\n",
        "  newWeight = []\n",
        "  i = 0\n",
        "  notTrainedWeight = modelNotTrained.get_weights()\n",
        "  for weight in modelNotTrained.get_weights():\n",
        "    newWeight.append( weight - (gradient[i] * clientweight) )\n",
        "    i += 1\n",
        "  modelNotTrained.set_weights(newWeight)\n",
        "\n",
        "#return random clients\n",
        "def returnRandomParticipatingClients(num_clients, num_participating_clients):\n",
        "  return random.sample(range(0, num_clients), num_participating_clients)\n",
        "\n",
        "#function that returns the accuracy score of the model on  the data\n",
        "def evaluateGlobalModel(glomodel, x, y):\n",
        "  return glomodel.evaluate(x, y, verbose=0)\n",
        "\n",
        "#function that returns the lambda_value according to the client accuracy score on local data\n",
        "def returnLambdaValueKD():\n",
        "  return 0.7"
      ],
      "metadata": {
        "id": "5YaHfQuq0bwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trainLeastTrainedClientsWithKD(num_participating_clients, clients_datasets_object, cluster_obj, representation_threshold, target_accuracy, xtrain, ytrain, xtest, ytest, classifierModel, training_rounds, num_epochs, validation_split, batch_size):\n",
        "\n",
        "  print(\"Dataset \", clients_datasets_obj_filename)\n",
        "  initial_evalution = evaluateGlobalModel(classifierModel, xtest, ytest)\n",
        "  global_accuracy = [initial_evalution[1]]\n",
        "  global_loss = [initial_evalution[0]]\n",
        "  print(\"Pretraining overall accuracy score = \", initial_evalution[1])\n",
        "  print()\n",
        "  num_clients = len(clients_datasets_object)\n",
        "\n",
        "  num_clients_training_each_round = []\n",
        "\n",
        "  training_history_obj = {}\n",
        "  for client_ID in range(num_clients):\n",
        "    training_history_obj[client_ID] = {\"training_num\": 0}\n",
        "\n",
        "  converging_round = 0\n",
        "  i = 0\n",
        "  while i < training_rounds:\n",
        "    #select random clients\n",
        "    participating_clients_IDs = returnRandomParticipatingClients(num_clients, num_participating_clients)\n",
        "\n",
        "\n",
        "    #####Displaying information at every round\n",
        "\n",
        "    #returns list of clients to train at each round, per cluster, and least trained whenever possible\n",
        "    selected_clients_IDs = returnLeastTrainedClientsFromClusters(participating_clients_IDs, cluster_obj, training_history_obj)\n",
        "\n",
        "    num_selected_clients = len(selected_clients_IDs)\n",
        "\n",
        "    sum_clients_weights = 0\n",
        "    for client_id in selected_clients_IDs:\n",
        "      client_weights = returnClientWeight(client_id, cluster_obj)\n",
        "      sum_clients_weights = sum_clients_weights + client_weights\n",
        "\n",
        "    current_round_clients_num_training = {}\n",
        "    for client_id in selected_clients_IDs:\n",
        "      current_round_clients_num_training[client_id] = training_history_obj[client_id][\"training_num\"]\n",
        "\n",
        "    ##################\n",
        "    #At client\n",
        "    ##################\n",
        "\n",
        "    selected_clients_grad_list = []\n",
        "\n",
        "    # global model trained only if the clients selected represents at least a minimum value of the global distribution\n",
        "    if sum_clients_weights >= representation_threshold:\n",
        "      i = i + 1\n",
        "\n",
        "      print(\"selected clients \", selected_clients_IDs)\n",
        "      print(\"training num\", current_round_clients_num_training)\n",
        "\n",
        "      #saving the counts of clients training at each round\n",
        "      num_clients_training_each_round.append(num_selected_clients)\n",
        "\n",
        "\n",
        "      for client_id in selected_clients_IDs:\n",
        "\n",
        "          #get client dataset\n",
        "          client_x, client_y = returnClientDataset(client_id, clients_datasets_object, xtrain, ytrain)\n",
        "\n",
        "          #####KD\n",
        "          ###BEGINING KD\n",
        "\n",
        "          # Build the teacher and student models\n",
        "          teacher_model = returnCopyGlobalModelToClient(classifierModel)\n",
        "          student_model = returnCopyGlobalModelToClient(classifierModel)\n",
        "\n",
        "          # Use teacher model to generate \"soft targets\" for the student\n",
        "          soft_target_train = teacher_model.predict(client_x)\n",
        "\n",
        "          lambda_val = returnLambdaValueKD()  # Adjust the weight lambda for the distillation loss\n",
        "\n",
        "          def student_knowledge_distillation_loss(y_true, y_pred):\n",
        "            y_true = tf.convert_to_tensor(y_true, dtype=tf.float32)\n",
        "\n",
        "            # Ensure that y_pred has the same shape as soft targets\n",
        "            y_soft = tf.convert_to_tensor(soft_target_train, dtype=tf.float32)\n",
        "            y_pred = tf.convert_to_tensor(y_pred, dtype=tf.float32)\n",
        "\n",
        "            loss_ce = losses.categorical_crossentropy(y_true, y_pred, from_logits=False)\n",
        "\n",
        "            loss_kd = tf.keras.losses.KLD(y_soft, tf.nn.softmax(y_pred / temperature))\n",
        "\n",
        "            return (1- lambda_val)*loss_ce + lambda_val * loss_kd  # Adjust the weight for the distillation loss as needed\n",
        "\n",
        "\n",
        "          #set student weights to central server weights and recompile\n",
        "          student_model.compile(loss=student_knowledge_distillation_loss, optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "          student_model.set_weights(teacher_model.get_weights())\n",
        "\n",
        "          #training model on client data\n",
        "          student_model.fit(client_x, client_y, epochs=num_epochs,\n",
        "                              validation_split = validation_split ,verbose=0,  batch_size=batch_size)\n",
        "\n",
        "\n",
        "          ###END KD\n",
        "          ######\n",
        "\n",
        "          #getting the gradient from the client\n",
        "          client_gradient = computeClientGradient(classifierModel, student_model)\n",
        "\n",
        "          #storing the gradients to be sent to central server\n",
        "          selected_clients_grad_list.append( {\"client_id\": client_id, \"clientgradient\": client_gradient} )\n",
        "\n",
        "          current_num_training = training_history_obj[client_id][\"training_num\"]\n",
        "          training_history_obj[client_id][\"training_num\"] = current_num_training + 1\n",
        "\n",
        "      ##################\n",
        "      #At central server\n",
        "      ##################\n",
        "\n",
        "      #updating the global model weights\n",
        "      for client_gradient in selected_clients_grad_list:\n",
        "\n",
        "          client_id = client_gradient[\"client_id\"]\n",
        "          client_x, client_y = returnClientDataset(client_id, clients_datasets_object, xtrain, ytrain)\n",
        "          client_weight = client_y.shape[0] / ytrain.shape[0]\n",
        "\n",
        "          addGradientNoCompression(classifierModel, client_gradient[\"clientgradient\"], client_weight)\n",
        "\n",
        "      selected_clients_grad_list = []\n",
        "\n",
        "      #evaluating the global model on data contained in central server\n",
        "      current_evalution = evaluateGlobalModel(classifierModel, xtest, ytest)\n",
        "      global_accuracy.append(current_evalution[1])\n",
        "      global_loss.append(current_evalution[0])\n",
        "\n",
        "      print(\"Round :\", i, \", selected clients :\", len(selected_clients_IDs), \"/\", len(cluster_obj), \" representing \", 100*sum_clients_weights, \"% of global distribution\" \", overall accuracy score = \", current_evalution[1])\n",
        "      print(\"lambda_val = \", lambda_val)\n",
        "      print()\n",
        "\n",
        "      if i % 5 == 0 :\n",
        "        print(\"accuracy = \", global_accuracy)\n",
        "        print(\"Average number of clients training the model per round : \", num_clients_training_each_round)\n",
        "        print()\n",
        "\n",
        "      if current_evalution[1] >= target_accuracy:\n",
        "        print(\"\")\n",
        "        print(\"Global model converged\")\n",
        "        print(\"Convergence accuracy score = \", current_evalution[1])\n",
        "        converging_round = i\n",
        "        break\n",
        "\n",
        "  #plot the results\n",
        "  print(\"Number of clients :\", num_clients)\n",
        "  print(\"Number of clients participating per round :\", num_participating_clients)\n",
        "  print(\"Number of clients training the model per round : \", num_clients_training_each_round)\n",
        "  print(\"Average number of clients training the model per round : \", np.array(num_clients_training_each_round).mean())\n",
        "  print(\"Number of iterations per client per round : \", num_epochs)\n",
        "  print(\"Number of rounds until convergence : \", converging_round)\n",
        "  print(\"training_history_obj\", training_history_obj)\n",
        "\n",
        "  #accuracy for each client dataset\n",
        "  clients_final_accuracy_list = []\n",
        "  for i in range(0, num_clients):\n",
        "    #get client dataset\n",
        "    client_x, client_y = returnClientDataset(i, clients_datasets_object, xtrain, ytrain)\n",
        "    client_score = evaluateGlobalModel(classifierModel, client_x, client_y)\n",
        "    clients_final_accuracy_list.append( client_score[1])\n",
        "  print(\"clients_final_accuracy_list\", clients_final_accuracy_list)\n",
        "\n",
        "  print (global_accuracy)\n",
        "  print (global_loss)\n",
        "  plt.plot(global_accuracy)\n",
        "  plt.ylabel('Accuracy per round')\n",
        "  plt.show()\n",
        "\n",
        "  plt.plot(global_loss)\n",
        "  plt.ylabel('Loss per round')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "6pMoPr4A7hnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def returnLeastTrainedClientsFromClusters(participating_clients_IDs, cluster_obj, clients_training_history_obj):\n",
        "  selected_clients_IDs = []\n",
        "  selected_cluster_IDs = []\n",
        "\n",
        "  for client_id in participating_clients_IDs:\n",
        "    cluster_ID = returnClusterID(client_id, cluster_obj)\n",
        "\n",
        "    if cluster_ID not in selected_cluster_IDs:\n",
        "      cluster_clients_list = returnClientsListsFromCluster(cluster_ID, cluster_obj )\n",
        "      candidates_clients_least  = [clientID for clientID in participating_clients_IDs if clientID in cluster_clients_list]\n",
        "      least_trained_client = client_id\n",
        "      client_trained_num = clients_training_history_obj[client_id][\"training_num\"]\n",
        "      for clientID in candidates_clients_least:\n",
        "        if clients_training_history_obj[clientID][\"training_num\"] < client_trained_num:\n",
        "          least_trained_client = clientID\n",
        "          client_trained_num = clients_training_history_obj[clientID][\"training_num\"]\n",
        "\n",
        "      selected_clients_IDs.append(least_trained_client)\n",
        "      selected_cluster_IDs.append(cluster_ID)\n",
        "\n",
        "  return selected_clients_IDs\n",
        "\n",
        "#return selected clients according to clusters\n",
        "def returnSelectedClientsFromClusters(participating_clients_IDs, cluster_obj):\n",
        "  selected_clients_IDs = []\n",
        "  selected_cluster_IDs = []\n",
        "  for client_id in participating_clients_IDs:\n",
        "    cluster_ID = returnClusterID(client_id, cluster_obj)\n",
        "    if cluster_ID not in selected_cluster_IDs:\n",
        "      selected_clients_IDs.append(client_id)\n",
        "      selected_cluster_IDs.append(cluster_ID)\n",
        "  return selected_clients_IDs\n",
        "\n",
        "def returnClientsListsFromCluster(client_id, cluster_obj ):\n",
        "  for attr, value in cluster_obj.items():\n",
        "    if client_id == attr:\n",
        "      return cluster_obj[attr][\"cluster\"]\n",
        "  return []"
      ],
      "metadata": {
        "id": "fl41P-0X8dUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data pre-processing + client clustering**"
      ],
      "metadata": {
        "id": "rkkYI58P2-fF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "number_clients = 100\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "clients_datasets_obj_filename = \"MNIST_100_clients_90percent_main_class.pickle\"\n",
        "\n",
        "clients_datasets_obj = pickle.load( open(clients_datasets_obj_filename, \"rb\" ) )"
      ],
      "metadata": {
        "id": "fNXRnEe40NMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model / data parameters\n",
        "num_classes = 10\n",
        "input_shape = (28, 28, 1)\n",
        "\n",
        "# Load the data and split it between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Scale images to the [0, 1] range\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "# Make sure images have shape (28, 28, 1)\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(x_train.shape[0], \"train samples\")\n",
        "print(x_test.shape[0], \"test samples\")\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "metadata": {
        "id": "Acw2WBxl27x7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_iterations_per_round = 1\n",
        "validation_size = 0.1\n",
        "\n",
        "central_server_model = returnInitialGlobalModel()\n",
        "clients_gradients = {}\n",
        "\n",
        "for i in range(len(clients_datasets_obj)):\n",
        "\n",
        "  #get client dataset\n",
        "  client_x, client_y = returnClientDataset(i, clients_datasets_obj, x_train, y_train)\n",
        "\n",
        "  #sending copy of global model to client\n",
        "  client_model = returnCopyGlobalModelToClient(central_server_model)\n",
        "\n",
        "  #training model on client data\n",
        "  client_model.fit(client_x, client_y, epochs = num_iterations_per_round,\n",
        "          validation_split = validation_size, verbose=0)\n",
        "\n",
        "  #getting the gradient from the client\n",
        "  client_gradient = computeClientGradient(central_server_model, client_model)\n",
        "\n",
        "  clients_gradients[i] = {\"client_id\": i , \"client_gradient\": client_gradient, \"client_weights\": client_model.get_weights()}"
      ],
      "metadata": {
        "id": "JVcJcSdz3CMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Getting the cosine similarity matrix between clients**"
      ],
      "metadata": {
        "id": "F4JNUe-B3Kt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similarity_matrix = []\n",
        "for i in range(len(clients_gradients)):\n",
        "  similarity_matrix.append([0] * len(clients_gradients))\n",
        "\n",
        "for i in range(len(clients_gradients)):\n",
        "  weights_i = returnClientGradientAsVector(clients_gradients[i][\"client_gradient\"])\n",
        "  similarity_matrix[i][i] = 1\n",
        "  j = i + 1\n",
        "  while j < len(clients_gradients):\n",
        "    weights_j = returnClientGradientAsVector(clients_gradients[j][\"client_gradient\"])\n",
        "    cosinesimilarityij = cosine_similarity([weights_i], [weights_j])\n",
        "    similarity_matrix[i][j] = cosinesimilarityij[0][0]\n",
        "    similarity_matrix[j][i] = cosinesimilarityij[0][0]\n",
        "    j = j + 1"
      ],
      "metadata": {
        "id": "AbT6GZW83E1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clients_clustered = {}\n",
        "\n",
        "similarity_threshold = 0.8\n",
        "\n",
        "all_clients_dataset_size = 0\n",
        "\n",
        "for i in range(len(clients_gradients)):\n",
        "  all_clients_dataset_size = all_clients_dataset_size + len(clients_datasets_obj[i][\"indexes\"])\n",
        "  if returnClusterID(i, clients_clustered) < 0:\n",
        "    clients_data_size = len(clients_datasets_obj[i][\"indexes\"])\n",
        "    clients_clustered[i] = {\"main_class\": clients_datasets_obj[i][\"main_class\"], \"cluster\": [i], \"cluster_size\": clients_data_size }\n",
        "  j = i + 1\n",
        "  cluster_id = returnClusterID(i, clients_clustered)\n",
        "  while j < len(clients_gradients):\n",
        "    if similarity_matrix[i][j] > similarity_threshold:\n",
        "      if returnClusterID(j, clients_clustered) < 0:\n",
        "        prev_cluster = clients_clustered[cluster_id][\"cluster\"]\n",
        "        prev_cluster.append(j)\n",
        "        clients_clustered[cluster_id][\"cluster\"] = prev_cluster\n",
        "        prev_cluster_size = clients_clustered[cluster_id][\"cluster_size\"]\n",
        "        clients_clustered[cluster_id][\"cluster_size\"] = prev_cluster_size + len(clients_datasets_obj[j][\"indexes\"])\n",
        "    j = j + 1\n",
        "\n",
        "for attr, value in clients_clustered.items():\n",
        "  cluster_size = clients_clustered[attr][\"cluster_size\"]\n",
        "  clients_clustered[attr][\"cluster_size\"] = cluster_size / all_clients_dataset_size\n",
        "\n",
        "\n",
        "#resetting unused parameters\n",
        "clients_gradients = {}\n",
        "similarity_matrix = []\n",
        "\n",
        "print(\"Total samples \", all_clients_dataset_size)\n",
        "print(\"Clusters of clients\")\n",
        "print(\"num of clusters\", len(clients_clustered))\n",
        "#clients_clustered"
      ],
      "metadata": {
        "id": "MnLRJNhE3JsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Settings experiments parameters**"
      ],
      "metadata": {
        "id": "uPt0kRJN3Sfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define temperature parameter for knowledge distillation\n",
        "temperature = 5\n",
        "\n",
        "participating_clients = 25\n",
        "selected_clients = len(clients_clustered)\n",
        "targeted_accuracy = 0.95\n",
        "max_training_rounds = 100\n",
        "epochs_per_client = 2\n",
        "validation_size = 0.1\n",
        "batch_size = 1\n",
        "\n",
        "distribution_representation_min = 0.89 #threshold for the selected clients to represent the global distribution in Client Selection Algorithm"
      ],
      "metadata": {
        "id": "KgKu2AfR3VIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_per_client = 2\n",
        "distribution_representation_min = 0.91\n",
        "participating_clients = 25\n",
        "lambda_val = 0.3\n",
        "\n",
        "starting_central_server_model = returnCopyGlobalModelToClient(central_server_model)\n",
        "\n",
        "trainLeastTrainedClientsWithKD(participating_clients, clients_datasets_obj, clients_clustered, distribution_representation_min, targeted_accuracy, x_train, y_train, x_test, y_test, starting_central_server_model, max_training_rounds, epochs_per_client, validation_size, batch_size)"
      ],
      "metadata": {
        "id": "EAUtPzAH3azR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}